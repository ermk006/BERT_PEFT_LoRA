{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ermk006/BERT_PEFT_LoRA/blob/main/Japanese_AI_vs_Human_Classifier_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "9a7b388d",
      "cell_type": "markdown",
      "source": [
        "# ðŸ‡¯ðŸ‡µ Japanese AI vs Human Classifier â€” Colab Notebook\n",
        "ã“ã®ãƒŽãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ **æ—¥æœ¬èªžçŸ­æ–‡ï¼ˆ50â€“300å­—ï¼‰** ã‚’ **AIç”Ÿæˆ(1) / äººé–“(0)** ã«2å€¤åˆ†é¡žã™ã‚‹å­¦ç¿’ãƒ†ãƒ³ãƒ—ãƒ¬ã§ã™ã€‚  \n",
        "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¯ Hugging Face Transformers + Datasets + PEFT(LoRA)ã€‚GPUãƒ©ãƒ³ã‚¿ã‚¤ãƒ æŽ¨å¥¨ï¼ˆColabã®ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã€â†’ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ã€â†’ GPUï¼‰ã€‚\n"
      ],
      "metadata": {
        "id": "9a7b388d"
      }
    },
    {
      "id": "5b799369",
      "cell_type": "code",
      "metadata": {
        "id": "5b799369"
      },
      "execution_count": null,
      "source": [
        "#@title 1) ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install transformers>=4.41.0 datasets>=2.20.0 accelerate>=0.30.0 peft>=0.11.0 scikit-learn>=1.3.0 pandas>=2.0.0 numpy>=1.24.0\n",
        "# Install fugashi for MecabTokenizer\n",
        "!pip -q install mecab-python3 unidic-lite\n",
        "!pip -q install fugashi ipadic\n",
        "\n",
        "import torch, platform\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| Python:\", platform.python_version())"
      ],
      "outputs": []
    },
    {
      "id": "8f95e920",
      "cell_type": "code",
      "metadata": {
        "id": "8f95e920"
      },
      "execution_count": null,
      "source": [
        "#@title 2) Google Drive ã‚’ãƒžã‚¦ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰\n",
        "# ãƒ‰ãƒ©ã‚¤ãƒ–ä¸Šã®CSVï¼ˆtrain.csv / valid.csvï¼‰ã‚’ä½¿ã†å ´åˆã«å®Ÿè¡Œ\n",
        "use_drive = False  #@param {type:\"boolean\"}\n",
        "if use_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Drive mounted at /content/drive\")"
      ],
      "outputs": []
    },
    {
      "id": "617c1bca",
      "cell_type": "code",
      "metadata": {
        "id": "617c1bca"
      },
      "execution_count": null,
      "source": [
        "#@title 3) ãƒ‡ãƒ¼ã‚¿ã®å ´æ‰€ã‚’æŒ‡å®š\n",
        "# å½¢å¼: CSV with columns: text,labelï¼ˆ0=human, 1=aiï¼‰\n",
        "# ä¾‹ï¼‰/content/train.csv, /content/valid.csv\n",
        "train_csv = \"/content/train.csv\"  #@param {type:\"string\"}\n",
        "valid_csv = \"/content/valid.csv\"  #@param {type:\"string\"}"
      ],
      "outputs": []
    },
    {
      "id": "f732d133",
      "cell_type": "code",
      "metadata": {
        "id": "f732d133"
      },
      "execution_count": null,
      "source": [
        "#@title 4) å­¦ç¿’è¨­å®šã¨å®Ÿè¡Œï¼ˆLoRAæŽ¨å¥¨ï¼‰\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
        "                          DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback)\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "import numpy as np, json, os, torch, torch.nn as nn\n",
        "\n",
        "model_name_or_path = \"cl-tohoku/bert-base-japanese-v3\"  #@param [\"cl-tohoku/bert-base-japanese-v3\",\"ku-nlp/deberta-v3-base-japanese\",\"rinna/japanese-roberta-base\",\"xlm-roberta-large\"] {allow-input: true}\n",
        "output_dir = \"/content/out/bertv3_ai_detector\"  #@param {type:\"string\"}\n",
        "use_lora = True  #@param {type:\"boolean\"}\n",
        "lora_r = 16  #@param {type:\"integer\"}\n",
        "lora_alpha = 32  #@param {type:\"integer\"}\n",
        "lora_dropout = 0.05  #@param {type:\"number\"}\n",
        "max_length = 256  #@param {type:\"integer\"}\n",
        "num_train_epochs = 4  #@param {type:\"integer\"}\n",
        "train_bs = 16  #@param {type:\"integer\"}\n",
        "eval_bs = 32  #@param {type:\"integer\"}\n",
        "learning_rate = 5e-5  #@param {type:\"number\"}\n",
        "early_stopping_patience = 2  #@param {type:\"integer\"}\n",
        "tune_threshold = True  #@param {type:\"boolean\"}\n",
        "seed = 42  #@param {type:\"integer\"}\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load datasets\n",
        "raw = load_dataset(\"csv\", data_files={\"train\": train_csv, \"validation\": valid_csv})\n",
        "tok = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token if getattr(tok, \"eos_token\", None) else tok.unk_token\n",
        "\n",
        "def preprocess(ex):\n",
        "    return tok(ex[\"text\"], truncation=True, max_length=max_length)\n",
        "\n",
        "tokenized = raw.map(preprocess, batched=True, remove_columns=[c for c in raw[\"train\"].column_names if c not in [\"text\",\"label\"]])\n",
        "tokenized = tokenized.rename_column(\"label\",\"labels\")\n",
        "\n",
        "cfg = AutoConfig.from_pretrained(model_name_or_path, num_labels=2)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=cfg)\n",
        "\n",
        "if use_lora:\n",
        "    target_modules = [\"query\",\"value\"] if \"bert\" in (cfg.model_type or \"\") else ([\"q_proj\",\"v_proj\"] if \"roberta\" in cfg.model_type or \"xlm\" in cfg.model_type else [\"query_proj\",\"value_proj\"])\n",
        "    lcfg = LoraConfig(task_type=TaskType.SEQ_CLS, r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, bias=\"none\", target_modules=target_modules)\n",
        "    model = get_peft_model(model, lcfg)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "# class weights (inverse frequency, normalized)\n",
        "y = np.array(tokenized[\"train\"][\"labels\"])\n",
        "w0 = 1.0 / max((y==0).sum(), 1); w1 = 1.0 / max((y==1).sum(), 1)\n",
        "s = (w0+w1)/2.0\n",
        "class_weights = torch.tensor([w0/s, w1/s], dtype=torch.float)\n",
        "\n",
        "def sm(x, axis=-1):\n",
        "    x = x - np.max(x, axis=axis, keepdims=True)\n",
        "    ex = np.exp(x); return ex/np.sum(ex, axis=axis, keepdims=True)\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    y_true = p.label_ids\n",
        "    prob = sm(logits, axis=1)[:,1]\n",
        "    y_pred = (prob >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, prob)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    return {\"accuracy\":acc,\"precision\":pr,\"recall\":rc,\"f1\":f1,\"auroc\":auc}\n",
        "\n",
        "class WeightedCETrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**{k:v for k,v in inputs.items() if k!=\"labels\"})\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device) if self.class_weights is not None else None)\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir, learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=train_bs, per_device_eval_batch_size=eval_bs,\n",
        "    num_train_epochs=num_train_epochs, weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"f1\", greater_is_better=True,\n",
        "    warmup_ratio=0.06, lr_scheduler_type=\"linear\", logging_steps=50, save_total_limit=2,\n",
        "    seed=seed, fp16=True, report_to=[\"none\"]\n",
        ")\n",
        "collator = DataCollatorWithPadding(tokenizer=tok)\n",
        "trainer = WeightedCETrainer(model=model, args=args, train_dataset=tokenized[\"train\"],\n",
        "                            eval_dataset=tokenized[\"validation\"],\n",
        "                            tokenizer=tok, data_collator=collator,\n",
        "                            compute_metrics=compute_metrics, class_weights=class_weights,\n",
        "                            callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)])\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model (including adapter weights and config)\n",
        "trainer.save_model(output_dir)\n",
        "\n",
        "\n",
        "# Evaluate & threshold tuning\n",
        "best_threshold = 0.5\n",
        "preds = trainer.predict(tokenized[\"validation\"])\n",
        "logits = preds.predictions[0] if isinstance(preds.predictions, tuple) else preds.predictions\n",
        "y_true = preds.label_ids\n",
        "prob1 = sm(logits, axis=1)[:,1]\n",
        "if tune_threshold:\n",
        "    import numpy as np\n",
        "    def pick_best_threshold(y_true, prob1):\n",
        "        best_t, best_f1 = 0.5, -1.0\n",
        "        for t in np.linspace(0.01, 0.99, 99):\n",
        "            p = (prob1 >= t).astype(int)\n",
        "            _,_,f1,_ = precision_recall_fscore_support(y_true, p, average=\"binary\", zero_division=0)\n",
        "            if f1 > best_f1: best_t, best_f1 = t, f1\n",
        "        return float(best_t)\n",
        "    best_threshold = pick_best_threshold(y_true, prob1)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "y_pred = (prob1 >= best_threshold).astype(int)\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "try:\n",
        "    auc = roc_auc_score(y_true, prob1)\n",
        "except Exception:\n",
        "    auc = float(\"nan\")\n",
        "\n",
        "report = {\n",
        "    \"best_threshold\": float(best_threshold),\n",
        "    \"metrics\": {\"accuracy\": float(acc), \"precision\": float(pr), \"recall\": float(rc), \"f1\": float(f1), \"auroc\": float(auc)}\n",
        "}\n",
        "with open(os.path.join(output_dir, \"eval_report.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "tok.save_pretrained(output_dir)\n",
        "print(\"Done. Saved to:\", output_dir)\n",
        "print(json.dumps(report, ensure_ascii=False, indent=2))"
      ],
      "outputs": []
    },
    {
      "id": "42af381d",
      "cell_type": "code",
      "metadata": {
        "id": "42af381d"
      },
      "execution_count": null,
      "source": [
        "#@title 5) æŽ¨è«–ãƒ‡ãƒ¢\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from peft import PeftModel\n",
        "import torch, torch.nn.functional as F, json, os, numpy as np\n",
        "\n",
        "model_dir = \"/content/out/bertv3_ai_detector\"  #@param {type:\"string\"}\n",
        "texts = [\"ã“ã‚Œã¯AIãŒä½œæˆã—ãŸæ–‡ç« ã§ã™ã€‚\",\"ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚\"]  #@param {type:\"raw\"}\n",
        "base_model_name_or_path = \"cl-tohoku/bert-base-japanese-v3\" # Base model name\n",
        "\n",
        "# é–¾å€¤ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "threshold = 0.5\n",
        "for name in [\"eval_report.json\",\"meta.json\",\"adapter_meta.json\"]:\n",
        "    p = os.path.join(model_dir, name)\n",
        "    if os.path.exists(p):\n",
        "        try:\n",
        "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "                j = json.load(f)\n",
        "            if \"best_threshold\" in j:\n",
        "                threshold = float(j[\"best_threshold\"]); break\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
        "# Load base model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(base_model_name_or_path, num_labels=2)\n",
        "# Load adapter weights\n",
        "model = PeftModel.from_pretrained(model, model_dir, from_adapter=True).eval()\n",
        "\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token if getattr(tok, \"eos_token\", None) else tok.unk_token\n",
        "\n",
        "enc = tok(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**enc).logits\n",
        "    prob = F.softmax(logits, dim=-1).cpu().numpy()[:,1]\n",
        "\n",
        "preds = (prob >= threshold).astype(int)\n",
        "for t, p, pr in zip(texts, preds, prob):\n",
        "    print(f\"[{'ai' if p==1 else 'human'}] p(ai)={pr:.3f}  text={t}\")"
      ],
      "outputs": []
    }
  ]
}